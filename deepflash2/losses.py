# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/05_losses.ipynb (unless otherwise specified).

__all__ = ['LOSSES', 'TorchLoss', 'WeightedLoss', 'JointLoss', 'WeightedSoftmaxCrossEntropy', 'get_loss']

# Cell
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.modules.loss import _Loss
from fastai.torch_core import TensorBase
import segmentation_models_pytorch as smp
from .utils import import_package

# Cell
LOSSES = ['WeightedCrossEntropyLoss', 'CrossEntropyLoss', 'CrossEntropyDiceLoss',
           'DiceLoss', 'JaccardLoss', 'FocalLoss', 'LovaszLoss', 'TverskyLoss']

# Cell
class TorchLoss(_Loss):
    'Wrapper class around loss function for handling different tensor types.'
    def __init__(self, loss):
        super().__init__()
        self.loss = loss

    def _contiguous(self, x): return TensorBase(x.contiguous())

    def forward(self, *input):
        input = map(self._contiguous, input)
        return self.loss(*input) #

# Cell
# from https://github.com/BloodAxe/pytorch-toolbelt/blob/develop/pytorch_toolbelt/losses/joint_loss.py
class WeightedLoss(_Loss):
    '''
    Wrapper class around loss function that applies weighted with fixed factor.
    This class helps to balance multiple losses if they have different scales
    '''
    def __init__(self, loss, weight=1.0):
        super().__init__()
        self.loss = loss
        self.weight = weight

    def forward(self, *input):
        return self.loss(*input) * self.weight

class JointLoss(_Loss):
    'Wrap two loss functions into one. This class computes a weighted sum of two losses.'

    def __init__(self, first: nn.Module, second: nn.Module, first_weight=1.0, second_weight=1.0):
        super().__init__()
        self.first = WeightedLoss(first, first_weight)
        self.second = WeightedLoss(second, second_weight)

    def forward(self, *input):
        return self.first(*input) + self.second(*input)

# Cell
class WeightedSoftmaxCrossEntropy(torch.nn.Module):
    "Weighted Softmax Cross Entropy loss functions"
    def __init__(self, *args, axis=-1, reduction = 'mean'):
        super().__init__()
        self.reduction = reduction
        self.axis = axis

    def forward(self, inp, targ, weights):
        # Weighted soft-max cross-entropy loss
        loss = F.cross_entropy(inp, targ, reduction='none')
        loss = loss * weights
        if  self.reduction == 'mean':
            return loss.mean()

        elif self.reduction == 'sum':
            return loss.sum()

        else:
            return loss

    def decodes(self, x): return x.argmax(dim=self.axis)
    def activation(self, x): return F.softmax(x, dim=self.axis)

# Cell
def get_loss(loss_name, mode='multiclass', classes=[1], smooth_factor=0., alpha=0.5, beta=0.5, gamma=2.0, reduction='mean', **kwargs):
    'Load losses from based on loss_name'

    assert loss_name in LOSSES, f'Select one of {LOSSES}'

    if loss_name=="WeightedCrossEntropyLoss":
        loss = WeightedSoftmaxCrossEntropy(axis=-1, reduction=reduction)

    else:
        if loss_name=="CrossEntropyLoss":
            loss = smp.losses.SoftCrossEntropyLoss(smooth_factor=smooth_factor, **kwargs)

        elif loss_name=="DiceLoss":
            loss = smp.losses.DiceLoss(mode=mode, classes=classes, **kwargs)

        elif loss_name=="JaccardLoss":
            loss = smp.losses.JaccardLoss(mode=mode, classes=classes, **kwargs)

        elif loss_name=="FocalLoss":
            loss = smp.losses.FocalLoss(mode=mode, alpha=alpha, gamma=gamma, reduction=reduction, **kwargs)

        elif loss_name=="LovaszLoss":
            loss = smp.losses.LovaszLoss(mode=mode, **kwargs)

        elif loss_name=="TverskyLoss":
            kornia = import_package('kornia')
            loss = kornia.losses.TverskyLoss(alpha=alpha, beta=beta, **kwargs)

        elif loss_name=="CrossEntropyDiceLoss":
            dc = smp.losses.DiceLoss(mode=mode, classes=classes, **kwargs)
            ce = smp.losses.SoftCrossEntropyLoss(smooth_factor=smooth_factor, **kwargs)
            loss = JointLoss(ce, dc, 1, 1)

    return TorchLoss(loss)
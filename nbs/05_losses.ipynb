{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp losses\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses\n",
    "\n",
    "> Implements popular segmentation loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastcore.test import *\n",
    "from fastai.torch_core import TensorImage, TensorMask\n",
    "from fastai.losses import CrossEntropyLossFlat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from fastai.torch_core import TensorBase\n",
    "import segmentation_models_pytorch as smp\n",
    "from deepflash2.utils import import_package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Losses implemented here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "LOSSES = ['WeightedCrossEntropyLoss', 'CrossEntropyLoss', 'CrossEntropyDiceLoss', \n",
    "           'DiceLoss', 'JaccardLoss', 'FocalLoss', 'LovaszLoss', 'TverskyLoss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Wrapper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper for handling different tensor types from [fastai](https://docs.fast.ai/torch_core.html#TensorBase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class TorchLoss(_Loss):\n",
    "    'Wrapper class around loss function for handling different tensor types.'\n",
    "    def __init__(self, loss):\n",
    "        super().__init__()\n",
    "        self.loss = loss\n",
    "        \n",
    "    def _contiguous(self, x): return TensorBase(x.contiguous())\n",
    "    \n",
    "    def forward(self, *input):\n",
    "        input = map(self._contiguous, input)        \n",
    "        return self.loss(*input) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper for combining different losses from [pytorch-toolbelt](https://github.com/BloodAxe/pytorch-toolbelt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "# from https://github.com/BloodAxe/pytorch-toolbelt/blob/develop/pytorch_toolbelt/losses/joint_loss.py\n",
    "class WeightedLoss(_Loss):\n",
    "    '''\n",
    "    Wrapper class around loss function that applies weighted with fixed factor.\n",
    "    This class helps to balance multiple losses if they have different scales\n",
    "    '''\n",
    "    def __init__(self, loss, weight=1.0):\n",
    "        super().__init__()\n",
    "        self.loss = loss\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, *input):\n",
    "        return self.loss(*input) * self.weight\n",
    "\n",
    "class JointLoss(_Loss):\n",
    "    'Wrap two loss functions into one. This class computes a weighted sum of two losses.'\n",
    "\n",
    "    def __init__(self, first: nn.Module, second: nn.Module, first_weight=1.0, second_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.first = WeightedLoss(first, first_weight)\n",
    "        self.second = WeightedLoss(second, second_weight)\n",
    "\n",
    "    def forward(self, *input):\n",
    "        return self.first(*input) + self.second(*input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Softmax Cross Entropy Loss\n",
    "\n",
    "as described by Falk, Thorsten, et al. \"U-Net: deep learning for cell counting, detection, and morphometry.\" Nature methods 16.1 (2019): 67-70.\n",
    "\n",
    "\n",
    "- `axis` for softmax calculations. Defaulted at 1 (channel dimension).\n",
    "- `reduction` will be used when we call `Learner.get_preds`\n",
    "- `activation` function will be applied on the raw output logits of the model when calling `Learner.get_preds` or `Learner.predict`\n",
    "- `decodes` function converts the output of the model to a format similar to the target (here binary masks). This is used in `Learner.predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class WeightedSoftmaxCrossEntropy(torch.nn.Module):\n",
    "    \"Weighted Softmax Cross Entropy loss functions\"\n",
    "    def __init__(self, *args, axis=-1, reduction = 'mean'):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.axis = axis\n",
    "    \n",
    "    def forward(self, inp, targ, weights):\n",
    "        # Weighted soft-max cross-entropy loss\n",
    "        loss = F.cross_entropy(inp, targ, reduction='none')\n",
    "        loss = loss * weights\n",
    "        if  self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "    def decodes(self, x): return x.argmax(dim=self.axis)\n",
    "    def activation(self, x): return F.softmax(x, dim=self.axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a segmentation task, we want to take the softmax over the channel dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare WeightedSoftmaxCrossEntropy loss with weights==1 to cross entropy loss\n",
    "n_classes = 5\n",
    "wce = TorchLoss(WeightedSoftmaxCrossEntropy(axis=1))\n",
    "ce = CrossEntropyLossFlat(axis=1)\n",
    "output = TensorImage(torch.randn(4, n_classes, 356, 356, requires_grad=True))\n",
    "target = TensorMask(torch.randint(0, n_classes, (4, 356, 356)))\n",
    "weights = torch.ones(4, 356, 356)\n",
    "test_close(wce(output, target, weights), ce(output, target), eps=1e-04)\n",
    "#Test WeightedSoftmaxCrossEntropy loss with weights!=1 is different than weights==1\n",
    "test_ne(wce(output, target, weights), wce(output, target, weights*0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmenation Models Pytorch Integration\n",
    "\n",
    "The `get_loss()` function loads popular segmentation losses from [Segmenation Models Pytorch](https://github.com/qubvel/segmentation_models.pytorch): \n",
    "- (Soft) CrossEntropy Loss (*insert citation*)\n",
    "- Dice Loss (*insert citation*)\n",
    "- Jaccard Loss (*insert citation*)\n",
    "- Focal Loss (*insert citation*)\n",
    "- Lovasz Loss (*insert citation*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kornia Segmentation Losses Integration\n",
    "\n",
    "The `get_loss()` function also loads segmentation losses from [kornia](https://github.com/kornia/kornia). \n",
    "Read the [docs](https://kornia.readthedocs.io/en/latest/losses.html#module) for a detailed explanation.\n",
    "- TverskyLoss (*insert citation*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def get_loss(loss_name, mode='multiclass', classes=[1], smooth_factor=0., alpha=0.5, beta=0.5, gamma=2.0, reduction='mean', **kwargs):\n",
    "    'Load losses from based on loss_name'\n",
    "    \n",
    "    assert loss_name in LOSSES, f'Select one of {LOSSES}'\n",
    "    \n",
    "    if loss_name==\"WeightedCrossEntropyLoss\": \n",
    "        loss = WeightedSoftmaxCrossEntropy(axis=-1, reduction=reduction)\n",
    "     \n",
    "    else:\n",
    "        if loss_name==\"CrossEntropyLoss\": \n",
    "            loss = smp.losses.SoftCrossEntropyLoss(smooth_factor=smooth_factor, **kwargs)\n",
    "\n",
    "        elif loss_name==\"DiceLoss\": \n",
    "            loss = smp.losses.DiceLoss(mode=mode, classes=classes, **kwargs)\n",
    "\n",
    "        elif loss_name==\"JaccardLoss\": \n",
    "            loss = smp.losses.JaccardLoss(mode=mode, classes=classes, **kwargs)\n",
    "\n",
    "        elif loss_name==\"FocalLoss\": \n",
    "            loss = smp.losses.FocalLoss(mode=mode, alpha=alpha, gamma=gamma, reduction=reduction, **kwargs)\n",
    "\n",
    "        elif loss_name==\"LovaszLoss\": \n",
    "            loss = smp.losses.LovaszLoss(mode=mode, **kwargs)\n",
    "        \n",
    "        elif loss_name==\"TverskyLoss\": \n",
    "            kornia = import_package('kornia')\n",
    "            loss = kornia.losses.TverskyLoss(alpha=alpha, beta=beta, **kwargs)\n",
    "        \n",
    "        elif loss_name==\"CrossEntropyDiceLoss\":\n",
    "            dc = smp.losses.DiceLoss(mode=mode, classes=classes, **kwargs)\n",
    "            ce = smp.losses.SoftCrossEntropyLoss(smooth_factor=smooth_factor, **kwargs)\n",
    "            loss = JointLoss(ce, dc, 1, 1)\n",
    "        \n",
    "    return TorchLoss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test if all losses are running\n",
    "for loss_name in LOSSES[1:]:\n",
    "    tst = get_loss(loss_name) \n",
    "    loss = tst(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare soft cross entropy loss with smooth_factor=0 to (fastai) cross entropy \n",
    "ce1 = get_loss('CrossEntropyLoss', smooth_factor=0)\n",
    "ce2 = CrossEntropyLossFlat(axis=1)\n",
    "test_close(ce1(output, target), ce2(output, target), eps=1e-04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare soft cross entropy loss with smooth_factor=0 to cross entropy \n",
    "jc = get_loss('JaccardLoss')\n",
    "dc = get_loss('DiceLoss')\n",
    "dc_loss = dc(output, target)\n",
    "dc_to_jc = 2*loss/(loss+1) #it seems to be the other way around?\n",
    "test_close(jc(output, target), dc_to_jc, eps=1e-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare TverskyLoss with alpha=0.5 and beta=0.5 to dice loss, should be equal\n",
    "tw = get_loss(\"TverskyLoss\", alpha=0.5, beta=0.5)\n",
    "test_close(dc(output, target), tw(output, target), eps=1e-02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_learner.ipynb.\n",
      "Converted 01_models.ipynb.\n",
      "Converted 02_data.ipynb.\n",
      "Converted 02a_transforms.ipynb.\n",
      "Converted 03_metrics.ipynb.\n",
      "Converted 04_callbacks.ipynb.\n",
      "Converted 05_losses.ipynb.\n",
      "Converted 06_utils.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
